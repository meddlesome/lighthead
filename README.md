# ğŸš€ Lighthead

A lightweight, TypeScript-powered headless web scraper that bypasses modern bot detection while maintaining minimal resource usage. Perfect for data extraction, content migration, and automated testing.

## âœ¨ Key Features

- **ğŸ›¡ï¸ Bot Detection Evasion** - Bypass Akamai, Cloudflare, and other protection systems
- **âš¡ High Performance** - Minimal footprint using `playwright-core`
- **ğŸ“ Multiple Formats** - Output as HTML, Markdown, or plain text
- **ğŸ“ Binary Support** - Download PDFs, images, and other files
- **ğŸ”— Smart Redirects** - Automatic redirect following with tracking
- **ğŸª Session Management** - Persistent cookie storage
- **ğŸ” Verbose Debugging** - Detailed HTTP request/response logging
- **ğŸŒ REST API** - Built-in Express server for integration
- **ğŸ“Š TypeScript** - Full type safety with comprehensive testing

## ğŸš€ Quick Start

```bash
# Install and setup
npm install
npx playwright install chromium

# Basic scraping
npm start -- https://example.com

# Advanced usage with stealth mode
npm start -- -v https://protected-site.com --format markdown --cookies auth.json
```

## ğŸ’» CLI Usage

### CLI Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `<url>` | string | **required** | Target URL to scrape |
| `--format <format>` | string | `html` | Output format: `html`, `markdown`, `text` |
| `--output <file>` | string | `stdout` | Save output to file instead of stdout |
| `--download` | flag | `false` | Download binary files (PDFs, images, etc.) |
| `--cookies <file>` | string | `none` | Load/save cookies from/to JSON file |
| `--no-redirects` | flag | `false` | Don't follow HTTP redirects (default: follow) |
| `--max-redirects <n>` | number | `10` | Maximum number of redirects to follow |
| `--no-stealth` | flag | `false` | Disable stealth mode (default: enabled) |
| `-v, --verbose` | flag | `false` | Enable verbose mode (show HTTP details) |
| `-h, --help` | flag | - | Show help message and exit |

### CLI Examples

```bash
# Output formats
npm start -- https://example.com --format markdown
npm start -- https://example.com --format text --output content.txt

# Binary file downloads
npm start -- https://example.com/document.pdf --download

# Session management
npm start -- https://example.com --cookies session.json

# Debugging and options
npm start -- -v https://example.com --no-redirects --max-redirects 5
```

## ğŸ›¡ï¸ Bot Detection Evasion

Lighthead bypasses modern protection systems through:

- **Browser Fingerprinting** - Realistic Chrome user agent and headers
- **JavaScript Stealth** - Hides automation indicators
- **Human Behavior** - Random mouse movements and delays
- **Proper Headers** - Sec-Fetch headers matching real browsers

âœ… **Tested Against:** Akamai Bot Manager, Cloudflare Bot Management, and more

## ğŸŒ REST API

Start the built-in server for easy integration:

```bash
# Production server
npm run server

# Development server (with auto-reload)
npm run server:dev

# Configure (optional)
cp .env.example .env
```

### ğŸ³ Docker Setup

Run Lighthead as a containerized service:

```bash
# Clone and setup
git clone <repository-url>
cd lighthead

# Configure environment (optional)
cp .env.example .env
# Edit .env to set PORT and API_KEY

# Build the image
docker compose build

# Run the service
docker compose up -d

# View logs
docker compose logs -f

# Stop the service
docker compose down
```

**Docker Environment Variables:**
- `PORT` - Server port (default: 3005)
- `API_KEY` - Optional API authentication key
- `NODE_ENV` - Set to `production` for production builds

### ğŸ”— Using with n8n

When using Lighthead with n8n in separate Docker networks, use `http://host.docker.internal:3005/scrape` in your n8n HTTP Request node instead of `http://localhost:3005/scrape`.

### Quick API Examples

```bash
# Basic scraping (returns markdown by default)
curl "http://localhost:3005/scrape?url=https://example.com"

# Get specific formats
curl "http://localhost:3005/scrape?url=https://example.com&format=html"
curl "http://localhost:3005/scrape?url=https://example.com&format=text"

# Download PDF as binary
curl "http://localhost:3005/scrape?url=https://example.com/doc.pdf&format=binary" -o doc.pdf
```

### API Query Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `url` | string | **required** | Target URL to scrape |
| `format` | string | `markdown` | Output format: `html`, `markdown`, `text`, `binary` |
| `followRedirects` | boolean | `true` | Follow HTTP redirects automatically (`true`/`false`) |
| `maxRedirects` | number | `10` | Maximum number of redirects to follow |
| `stealth` | boolean | `true` | Enable bot detection evasion (`true`/`false`) |

**Authentication (optional):**
- Add `X-API-Key` header or `apiKey` query parameter if `API_KEY` is set in environment

### API Response Formats

**Supported formats:** `html`, `markdown`, `text`, `binary`

- **Default:** `markdown` (changed from `json` for better usability)
- **All responses:** JSON format with content in the `output` field
- **Binary files:** Direct download when `format=binary`, metadata when no format specified

### Content Source Quality

All API responses include a `contentSource` field indicating the completeness of the extracted content:

| Value | Description | Quality Level |
|-------|-------------|---------------|
| `"full"` | All resources loaded successfully (complete JavaScript execution) | âœ… **Best** - Ideal for LLM analysis |
| `"partial"` | DOM loaded + some JavaScript, but some resources failed to load | âš ï¸ **Good** - Most content available |
| `"minimal"` | Basic DOM only, limited or no JavaScript execution | âš ï¸ **Basic** - Raw HTML content |

**Example API Response:**
```json
{
  "success": true,
  "status": "completed",
  "contentSource": "full",
  "outputLength": 1234,
  "output": "# Page Title\n\nContent here...",
  "format": "markdown",
  "finalUrl": "https://example.com",
  "redirectCount": 0
}
```

**Adaptive Loading Strategy:**
1. **First attempt:** `waitUntil: 'load'` (30s timeout) â†’ `contentSource: "full"`
2. **Fallback:** `waitUntil: 'domcontentloaded'` â†’ `contentSource: "partial"` 
3. **Final fallback:** Basic DOM extraction â†’ `contentSource: "minimal"`

## ğŸ¯ Use Cases

- **Web Scraping** - Extract content from protected websites
- **Content Migration** - Convert web pages to markdown
- **Monitoring** - Track website changes with session persistence
- **API Integration** - Use as a microservice in larger applications
- **Testing** - Validate website behavior under different conditions

## ğŸ”§ Development

```bash
# Setup
npm install
npm run build

# Development
npm run dev https://example.com
npm run server:dev  # API development server

# Testing
npm test              # Unit tests only
npm run test:integration  # Integration tests only  
npm run test:all      # All tests including integration
npm run test:coverage # Coverage report
npm run test:watch    # Watch mode for development
npm run lint          # TypeScript type checking
```

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Run `npm test` and `npm run lint`
5. Submit a pull request

---

â­ **Star this repo if you find it useful!**